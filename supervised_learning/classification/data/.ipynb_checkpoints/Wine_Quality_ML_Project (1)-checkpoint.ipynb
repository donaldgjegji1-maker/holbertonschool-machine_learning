{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Analysis and Machine Learning Project\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This comprehensive machine learning project analyzes the Wine Quality dataset to predict wine quality based on physicochemical properties. The dataset contains various chemical measurements of red wine samples along with quality ratings.\n",
    "\n",
    "### Objectives:\n",
    "- Perform exploratory data analysis (EDA) to understand the dataset\n",
    "- Clean and preprocess the data\n",
    "- Engineer meaningful features\n",
    "- Build and evaluate multiple machine learning models:\n",
    "  - **Regression**: Decision Tree, Random Forest\n",
    "  - **Classification**: KNN, Naive Bayes, Decision Tree, XGBoost with hyperparameter tuning\n",
    "  - **Unsupervised Learning**: Clustering and Dimensionality Reduction\n",
    "- Compare model performance and derive insights\n",
    "\n",
    "### Dataset Description:\n",
    "The Wine Quality dataset contains 11 physicochemical features:\n",
    "1. **Fixed Acidity**: Non-volatile acids (tartaric acid)\n",
    "2. **Volatile Acidity**: Amount of acetic acid (high levels = vinegar taste)\n",
    "3. **Citric Acid**: Adds freshness and flavor\n",
    "4. **Residual Sugar**: Sugar remaining after fermentation\n",
    "5. **Chlorides**: Amount of salt in wine\n",
    "6. **Free Sulfur Dioxide**: Prevents microbial growth\n",
    "7. **Total Sulfur Dioxide**: Free + bound forms of SO2\n",
    "8. **Density**: Density of wine (depends on alcohol and sugar)\n",
    "9. **pH**: Acidity/alkalinity scale (0-14)\n",
    "10. **Sulphates**: Wine additive (antimicrobial and antioxidant)\n",
    "11. **Alcohol**: Alcohol percentage by volume\n",
    "\n",
    "**Target Variable**: **Quality** - Score between 0 and 10 (integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries\n",
    "\n",
    "First, we'll import all necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                             mean_squared_error, r2_score, mean_absolute_error)\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Unsupervised Learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "Let's load the wine quality dataset and perform initial exploration to understand its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(\"=\"*50)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"=\"*50)\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"=\"*50)\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*50)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from Initial Exploration:\n",
    "- The dataset contains numeric features only (float64)\n",
    "- All features have different scales (e.g., alcohol ranges from 8-15%, while chlorides range from 0.01-0.6)\n",
    "- Quality is our target variable with discrete integer values\n",
    "- We need to check for missing values and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Cleaning\n",
    "\n",
    "Let's check for missing values, duplicates, and handle any data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\"*50)\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\"*50)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage.round(2)\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"\\nâœ… No missing values found!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Total missing values: {missing_df['Missing Count'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"Removing {duplicates} duplicate rows...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"âœ… Duplicates removed. New shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"âœ… No duplicate rows found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable (Quality)\n",
    "print(\"=\"*50)\n",
    "print(\"Quality Distribution:\")\n",
    "print(\"=\"*50)\n",
    "quality_counts = df['quality'].value_counts().sort_index()\n",
    "print(quality_counts)\n",
    "print(f\"\\nQuality Range: {df['quality'].min()} to {df['quality'].max()}\")\n",
    "print(f\"Mean Quality: {df['quality'].mean():.2f}\")\n",
    "print(f\"Median Quality: {df['quality'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Summary:\n",
    "- âœ… No missing values detected\n",
    "- âœ… Checked for duplicates\n",
    "- The dataset is clean and ready for analysis\n",
    "- Quality scores are imbalanced (most wines rated 5-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now let's visualize and analyze the data to uncover patterns and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quality distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "quality_counts.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Wine Quality Scores', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Quality Score', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(quality_counts, labels=quality_counts.index, autopct='%1.1f%%', \n",
    "            startangle=90, colors=plt.cm.Set3.colors)\n",
    "axes[1].set_title('Quality Score Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š The quality distribution shows that most wines are rated 5 or 6 (average quality).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of all features\n",
    "fig, axes = plt.subplots(4, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    axes[idx].hist(df[col], bins=30, color='teal', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{col.replace(\"_\", \" \").title()}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Feature distributions help us understand the data spread and identify potential outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show features most correlated with quality\n",
    "print(\"=\"*50)\n",
    "print(\"Features Most Correlated with Quality:\")\n",
    "print(\"=\"*50)\n",
    "quality_corr = correlation_matrix['quality'].sort_values(ascending=False)\n",
    "print(quality_corr[1:])  # Exclude quality itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlations with quality\n",
    "top_features = quality_corr[1:6].index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    axes[idx].scatter(df[feature], df['quality'], alpha=0.5, color='darkblue')\n",
    "    axes[idx].set_xlabel(feature.replace('_', ' ').title(), fontsize=11)\n",
    "    axes[idx].set_ylabel('Quality', fontsize=11)\n",
    "    axes[idx].set_title(f'{feature.replace(\"_\", \" \").title()} vs Quality\\nCorr: {quality_corr[feature]:.3f}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Box Plots for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots to identify outliers\n",
    "fig, axes = plt.subplots(4, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    axes[idx].boxplot(df[col], vert=True, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='lightblue', color='black'),\n",
    "                     medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(f'{col.replace(\"_\", \" \").title()}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Box plots reveal outliers in several features (points beyond the whiskers).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Feature Relationships by Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots - Feature distributions by quality\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "feature_cols = [col for col in df.columns if col != 'quality']\n",
    "\n",
    "for idx, col in enumerate(feature_cols):\n",
    "    sns.violinplot(data=df, x='quality', y=col, ax=axes[idx], palette='Set2')\n",
    "    axes[idx].set_title(f'{col.replace(\"_\", \" \").title()} by Quality', \n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Quality Score')\n",
    "    axes[idx].set_ylabel(col.replace('_', ' ').title())\n",
    "    axes[idx].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[11])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Violin plots show how feature distributions vary across different quality levels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key EDA Insights:\n",
    "- ðŸ· Most wines are rated 5 or 6 (average quality)\n",
    "- ðŸ“ˆ Alcohol content shows positive correlation with quality\n",
    "- ðŸ“‰ Volatile acidity shows negative correlation with quality\n",
    "- ðŸ” Several features contain outliers that we may need to handle\n",
    "- ðŸ”— Some features are highly correlated with each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Engineering\n",
    "\n",
    "Let's create new features that might improve our model's predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_engineered = df.copy()\n",
    "\n",
    "print(\"Creating new features...\\n\")\n",
    "\n",
    "# 1. Total Acidity (fixed + volatile + citric)\n",
    "df_engineered['total_acidity'] = (df_engineered['fixed_acidity'] + \n",
    "                                   df_engineered['volatile_acidity'] + \n",
    "                                   df_engineered['citric_acid'])\n",
    "\n",
    "# 2. Acidity Ratio\n",
    "df_engineered['acidity_ratio'] = df_engineered['fixed_acidity'] / (df_engineered['volatile_acidity'] + 0.001)\n",
    "\n",
    "# 3. Free SO2 Ratio\n",
    "df_engineered['free_so2_ratio'] = df_engineered['free_sulfur_dioxide'] / (df_engineered['total_sulfur_dioxide'] + 1)\n",
    "\n",
    "# 4. Alcohol to Density Ratio\n",
    "df_engineered['alcohol_density_ratio'] = df_engineered['alcohol'] / df_engineered['density']\n",
    "\n",
    "# 5. Sugar to Alcohol Ratio\n",
    "df_engineered['sugar_alcohol_ratio'] = df_engineered['residual_sugar'] / (df_engineered['alcohol'] + 0.001)\n",
    "\n",
    "# 6. Alcohol Category (binning)\n",
    "df_engineered['alcohol_category'] = pd.cut(df_engineered['alcohol'], \n",
    "                                            bins=[0, 10, 11.5, 15], \n",
    "                                            labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# 7. Quality Category (for classification)\n",
    "df_engineered['quality_category'] = pd.cut(df_engineered['quality'], \n",
    "                                            bins=[0, 5, 7, 10], \n",
    "                                            labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# 8. Sulphate to Chloride Ratio\n",
    "df_engineered['sulphate_chloride_ratio'] = df_engineered['sulphates'] / (df_engineered['chlorides'] + 0.001)\n",
    "\n",
    "# 9. pH Category\n",
    "df_engineered['pH_category'] = pd.cut(df_engineered['pH'], \n",
    "                                       bins=[0, 3.0, 3.3, 5], \n",
    "                                       labels=['Very_Acidic', 'Acidic', 'Less_Acidic'])\n",
    "\n",
    "# 10. Is High Quality (binary classification target)\n",
    "df_engineered['is_high_quality'] = (df_engineered['quality'] >= 6).astype(int)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"New Features Created:\")\n",
    "print(\"=\"*50)\n",
    "new_features = ['total_acidity', 'acidity_ratio', 'free_so2_ratio', \n",
    "                'alcohol_density_ratio', 'sugar_alcohol_ratio', \n",
    "                'sulphate_chloride_ratio']\n",
    "for feature in new_features:\n",
    "    print(f\"âœ… {feature}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset now has {df_engineered.shape[1]} columns (original: {df.shape[1]})\")\n",
    "print(f\"\\nNew shape: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of engineered features\n",
    "print(\"Sample of engineered features:\")\n",
    "df_engineered[['alcohol', 'density', 'alcohol_density_ratio', 'quality', 'quality_category', 'is_high_quality']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some engineered features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot 1: Total Acidity vs Quality\n",
    "axes[0].scatter(df_engineered['total_acidity'], df_engineered['quality'], alpha=0.5, color='purple')\n",
    "axes[0].set_xlabel('Total Acidity')\n",
    "axes[0].set_ylabel('Quality')\n",
    "axes[0].set_title('Total Acidity vs Quality', fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Acidity Ratio vs Quality\n",
    "axes[1].scatter(df_engineered['acidity_ratio'], df_engineered['quality'], alpha=0.5, color='green')\n",
    "axes[1].set_xlabel('Acidity Ratio')\n",
    "axes[1].set_ylabel('Quality')\n",
    "axes[1].set_title('Acidity Ratio vs Quality', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Alcohol-Density Ratio vs Quality\n",
    "axes[2].scatter(df_engineered['alcohol_density_ratio'], df_engineered['quality'], alpha=0.5, color='orange')\n",
    "axes[2].set_xlabel('Alcohol-Density Ratio')\n",
    "axes[2].set_ylabel('Quality')\n",
    "axes[2].set_title('Alcohol-Density Ratio vs Quality', fontweight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Alcohol Category Distribution\n",
    "df_engineered['alcohol_category'].value_counts().plot(kind='bar', ax=axes[3], color='steelblue', edgecolor='black')\n",
    "axes[3].set_title('Alcohol Category Distribution', fontweight='bold')\n",
    "axes[3].set_xlabel('Category')\n",
    "axes[3].set_ylabel('Count')\n",
    "axes[3].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Plot 5: Quality Category Distribution\n",
    "df_engineered['quality_category'].value_counts().plot(kind='bar', ax=axes[4], color='coral', edgecolor='black')\n",
    "axes[4].set_title('Quality Category Distribution', fontweight='bold')\n",
    "axes[4].set_xlabel('Category')\n",
    "axes[4].set_ylabel('Count')\n",
    "axes[4].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Plot 6: High Quality Distribution\n",
    "df_engineered['is_high_quality'].value_counts().plot(kind='bar', ax=axes[5], color='teal', edgecolor='black')\n",
    "axes[5].set_title('High Quality Wine Distribution', fontweight='bold')\n",
    "axes[5].set_xlabel('Is High Quality (0=No, 1=Yes)')\n",
    "axes[5].set_ylabel('Count')\n",
    "axes[5].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Engineered features provide new perspectives on wine quality relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Summary:\n",
    "- Created **ratio features** (acidity ratio, SO2 ratio, alcohol-density ratio)\n",
    "- Created **composite features** (total acidity)\n",
    "- Created **categorical features** for binning continuous variables\n",
    "- Created **binary target** for classification (is_high_quality)\n",
    "- These features capture domain knowledge about wine chemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Preparation for Modeling\n",
    "\n",
    "Prepare the data for machine learning by handling categorical variables, scaling features, and splitting into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features for modeling\n",
    "numeric_features = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n",
    "                   'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n",
    "                   'pH', 'sulphates', 'alcohol', 'total_acidity', 'acidity_ratio',\n",
    "                   'free_so2_ratio', 'alcohol_density_ratio', 'sugar_alcohol_ratio',\n",
    "                   'sulphate_chloride_ratio']\n",
    "\n",
    "# Prepare feature matrix and target variables\n",
    "X = df_engineered[numeric_features].copy()\n",
    "y_regression = df_engineered['quality'].copy()  # For regression\n",
    "y_classification = df_engineered['is_high_quality'].copy()  # For binary classification\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Data Preparation:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Regression target shape: {y_regression.shape}\")\n",
    "print(f\"Classification target shape: {y_classification.shape}\")\n",
    "print(f\"\\nNumber of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "print(\"\\nSplitting data into training (80%) and testing (20%) sets...\\n\")\n",
    "\n",
    "# For regression\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_regression, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# For classification\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X, y_classification, test_size=0.2, random_state=42, stratify=y_classification\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Train/Test Split Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training samples: {X_train_reg.shape[0]} ({X_train_reg.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"Testing samples: {X_test_reg.shape[0]} ({X_test_reg.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"\\nClassification target distribution in training set:\")\n",
    "print(y_train_clf.value_counts())\n",
    "print(f\"\\nClassification target distribution in testing set:\")\n",
    "print(y_test_clf.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "print(\"\\nPerforming feature scaling using StandardScaler...\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale regression data\n",
    "X_train_reg_scaled = scaler.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler.transform(X_test_reg)\n",
    "\n",
    "# Scale classification data (using same scaler)\n",
    "X_train_clf_scaled = scaler.fit_transform(X_train_clf)\n",
    "X_test_clf_scaled = scaler.transform(X_test_clf)\n",
    "\n",
    "print(\"âœ… Feature scaling completed!\")\n",
    "print(f\"\\nScaled features - Mean: ~0, Std: ~1\")\n",
    "print(f\"Sample scaled values (first 5 features, first sample):\")\n",
    "print(X_train_reg_scaled[0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Regression Models\n",
    "\n",
    "Let's build regression models to predict wine quality as a continuous value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Training Decision Tree Regressor...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train Decision Tree\n",
    "dt_reg = DecisionTreeRegressor(random_state=42, max_depth=10)\n",
    "dt_reg.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt_train = dt_reg.predict(X_train_reg_scaled)\n",
    "y_pred_dt_test = dt_reg.predict(X_test_reg_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  RÂ² Score: {r2_score(y_train_reg, y_pred_dt_train):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train_reg, y_pred_dt_train)):.4f}\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_train_reg, y_pred_dt_train):.4f}\")\n",
    "\n",
    "print(\"\\nTesting Set Performance:\")\n",
    "print(f\"  RÂ² Score: {r2_score(y_test_reg, y_pred_dt_test):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_dt_test)):.4f}\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_test_reg, y_pred_dt_test):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Decision Tree Regressor training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Training Random Forest Regressor...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=15, n_jobs=-1)\n",
    "rf_reg.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf_train = rf_reg.predict(X_train_reg_scaled)\n",
    "y_pred_rf_test = rf_reg.predict(X_test_reg_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  RÂ² Score: {r2_score(y_train_reg, y_pred_rf_train):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train_reg, y_pred_rf_train)):.4f}\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_train_reg, y_pred_rf_train):.4f}\")\n",
    "\n",
    "print(\"\\nTesting Set Performance:\")\n",
    "print(f\"  RÂ² Score: {r2_score(y_test_reg, y_pred_rf_test):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_rf_test)):.4f}\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_test_reg, y_pred_rf_test):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Random Forest Regressor training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': numeric_features,\n",
    "    'importance': rf_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'], color='forestgreen', edgecolor='black')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Random Forest - Feature Importance for Quality Prediction', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Regression Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regression models\n",
    "regression_results = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Random Forest'],\n",
    "    'Train RÂ²': [\n",
    "        r2_score(y_train_reg, y_pred_dt_train),\n",
    "        r2_score(y_train_reg, y_pred_rf_train)\n",
    "    ],\n",
    "    'Test RÂ²': [\n",
    "        r2_score(y_test_reg, y_pred_dt_test),\n",
    "        r2_score(y_test_reg, y_pred_rf_test)\n",
    "    ],\n",
    "    'Train RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_train_reg, y_pred_dt_train)),\n",
    "        np.sqrt(mean_squared_error(y_train_reg, y_pred_rf_train))\n",
    "    ],\n",
    "    'Test RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_test_reg, y_pred_dt_test)),\n",
    "        np.sqrt(mean_squared_error(y_test_reg, y_pred_rf_test))\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Regression Models Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(regression_results.to_string(index=False))\n",
    "print(\"\\nðŸ“Š Random Forest generally performs better with lower RMSE and better generalization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Decision Tree\n",
    "axes[0].scatter(y_test_reg, y_pred_dt_test, alpha=0.6, color='blue')\n",
    "axes[0].plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Quality', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Quality', fontsize=12)\n",
    "axes[0].set_title(f'Decision Tree\\nTest RÂ² = {r2_score(y_test_reg, y_pred_dt_test):.3f}', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Random Forest\n",
    "axes[1].scatter(y_test_reg, y_pred_rf_test, alpha=0.6, color='green')\n",
    "axes[1].plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Quality', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Quality', fontsize=12)\n",
    "axes[1].set_title(f'Random Forest\\nTest RÂ² = {r2_score(y_test_reg, y_pred_rf_test):.3f}', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Classification Models\n",
    "\n",
    "Now let's build classification models to predict if a wine is high quality (quality >= 6) or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Training K-Nearest Neighbors Classifier...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train KNN\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train_clf_scaled, y_train_clf)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn_train = knn_clf.predict(X_train_clf_scaled)\n",
    "y_pred_knn_test = knn_clf.predict(X_test_clf_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_train_clf, y_pred_knn_train):.4f}\")\n",
    "\n",
    "print(\"\\nTesting Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test_clf, y_pred_knn_test):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test_clf, y_pred_knn_test, target_names=['Low Quality', 'High Quality']))\n",
    "\n",
    "print(\"âœ… KNN Classifier training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Training Gaussian Naive Bayes Classifier...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train_clf_scaled, y_train_clf)\n",
    "\n",
    "# Predictions\n",
    "y_pred_nb_train = nb_clf.predict(X_train_clf_scaled)\n",
    "y_pred_nb_test = nb_clf.predict(X_test_clf_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_train_clf, y_pred_nb_train):.4f}\")\n",
    "\n",
    "print(\"\\nTesting Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test_clf, y_pred_nb_test):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test_clf, y_pred_nb_test, target_names=['Low Quality', 'High Quality']))\n",
    "\n",
    "print(\"âœ… Naive Bayes Classifier training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Training Decision Tree Classifier...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train Decision Tree\n",
    "dt_clf = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "dt_clf.fit(X_train_clf_scaled, y_train_clf)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt_clf_train = dt_clf.predict(X_train_clf_scaled)\n",
    "y_pred_dt_clf_test = dt_clf.predict(X_test_clf_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_train_clf, y_pred_dt_clf_train):.4f}\")\n",
    "\n",
    "print(\"\\nTesting Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test_clf, y_pred_dt_clf_test):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test_clf, y_pred_dt_clf_test, target_names=['Low Quality', 'High Quality']))\n",
    "\n",
    "print(\"âœ… Decision Tree Classifier training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Random Forest Classifier (with Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Training Random Forest Classifier with Hyperparameter Tuning...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Grid Search with Cross-Validation\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_clf_scaled, y_train_clf)\n",
    "\n",
    "# Best parameters\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Best model\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf_clf_train = best_rf_clf.predict(X_train_clf_scaled)\n",
    "y_pred_rf_clf_test = best_rf_clf.predict(X_test_clf_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_train_clf, y_pred_rf_clf_train):.4f}\")\n",
    "\n",
    "print(\"\\nTesting Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test_clf, y_pred_rf_clf_test):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test_clf, y_pred_rf_clf_test, target_names=['Low Quality', 'High Quality']))\n",
    "\n",
    "print(\"âœ… Random Forest Classifier with tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all classifiers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "models = [\n",
    "    ('KNN', y_pred_knn_test),\n",
    "    ('Naive Bayes', y_pred_nb_test),\n",
    "    ('Decision Tree', y_pred_dt_clf_test),\n",
    "    ('Random Forest (Tuned)', y_pred_rf_clf_test)\n",
    "]\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(models):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    cm = confusion_matrix(y_test_clf, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[row, col],\n",
    "                xticklabels=['Low Quality', 'High Quality'],\n",
    "                yticklabels=['Low Quality', 'High Quality'])\n",
    "    axes[row, col].set_title(f'{name}\\nAccuracy: {accuracy_score(y_test_clf, y_pred):.3f}', \n",
    "                            fontsize=13, fontweight='bold')\n",
    "    axes[row, col].set_ylabel('Actual')\n",
    "    axes[row, col].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Classification Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare classification models\n",
    "classification_results = pd.DataFrame({\n",
    "    'Model': ['KNN', 'Naive Bayes', 'Decision Tree', 'Random Forest (Tuned)'],\n",
    "    'Train Accuracy': [\n",
    "        accuracy_score(y_train_clf, y_pred_knn_train),\n",
    "        accuracy_score(y_train_clf, y_pred_nb_train),\n",
    "        accuracy_score(y_train_clf, y_pred_dt_clf_train),\n",
    "        accuracy_score(y_train_clf, y_pred_rf_clf_train)\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        accuracy_score(y_test_clf, y_pred_knn_test),\n",
    "        accuracy_score(y_test_clf, y_pred_nb_test),\n",
    "        accuracy_score(y_test_clf, y_pred_dt_clf_test),\n",
    "        accuracy_score(y_test_clf, y_pred_rf_clf_test)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Classification Models Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_results.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(classification_results))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, classification_results['Train Accuracy'], width, label='Train', color='skyblue', edgecolor='black')\n",
    "ax.bar(x + width/2, classification_results['Test Accuracy'], width, label='Test', color='coral', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Classification Models Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(classification_results['Model'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Random Forest with hyperparameter tuning typically achieves the best performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Unsupervised Learning\n",
    "\n",
    "Apply clustering and dimensionality reduction techniques to discover patterns in wine data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Performing K-Means Clustering...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_train_clf_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertias, marker='o', linewidth=2, markersize=8, color='darkblue')\n",
    "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
    "plt.ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "plt.title('Elbow Method for Optimal K', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š The elbow point suggests the optimal number of clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with optimal k (let's use k=3)\n",
    "optimal_k = 3\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_train_clf_scaled)\n",
    "\n",
    "print(f\"\\nK-Means clustering completed with K={optimal_k}\")\n",
    "print(f\"Cluster distribution:\")\n",
    "print(pd.Series(clusters).value_counts().sort_index())\n",
    "\n",
    "# Add clusters to dataframe for analysis\n",
    "X_train_with_clusters = X_train_clf.copy()\n",
    "X_train_with_clusters['cluster'] = clusters\n",
    "X_train_with_clusters['quality'] = y_train_reg.loc[X_train_clf.index].values\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Cluster Characteristics (Mean Values):\")\n",
    "print(\"=\"*50)\n",
    "cluster_means = X_train_with_clusters.groupby('cluster')[['alcohol', 'volatile_acidity', \n",
    "                                                           'sulphates', 'quality']].mean()\n",
    "print(cluster_means.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Performing Principal Component Analysis (PCA)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_train_clf_scaled)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"\\nNumber of original features: {X_train_clf_scaled.shape[1]}\")\n",
    "print(f\"Variance explained by first 2 components: {cumulative_variance[1]:.2%}\")\n",
    "print(f\"Variance explained by first 3 components: {cumulative_variance[2]:.2%}\")\n",
    "print(f\"Variance explained by first 5 components: {cumulative_variance[4]:.2%}\")\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, len(explained_variance) + 1), explained_variance, \n",
    "            color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "axes[0].set_title('Variance Explained by Each Component', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
    "             marker='o', linewidth=2, markersize=8, color='darkgreen')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "axes[1].set_title('Cumulative Explained Variance', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… PCA analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Visualize Clusters in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2 components for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_train_clf_scaled)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Colored by K-Means clusters\n",
    "scatter1 = axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], \n",
    "                          c=clusters, cmap='viridis', alpha=0.6, s=50, edgecolor='black')\n",
    "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
    "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
    "axes[0].set_title('Wine Samples in PCA Space\\nColored by K-Means Clusters', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Plot 2: Colored by actual quality\n",
    "quality_values = y_train_reg.loc[X_train_clf.index].values\n",
    "scatter2 = axes[1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], \n",
    "                          c=quality_values, cmap='RdYlGn', alpha=0.6, s=50, edgecolor='black')\n",
    "axes[1].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
    "axes[1].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
    "axes[1].set_title('Wine Samples in PCA Space\\nColored by Actual Quality', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Quality Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š PCA reduces dimensionality while preserving variance, making it easier to visualize clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 PCA Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PCA components\n",
    "components_df = pd.DataFrame(\n",
    "    pca_2d.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=numeric_features\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Feature Contributions to Principal Components:\")\n",
    "print(\"=\"*50)\n",
    "print(components_df.round(3))\n",
    "\n",
    "# Visualize component loadings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PC1 loadings\n",
    "pc1_sorted = components_df['PC1'].sort_values()\n",
    "axes[0].barh(range(len(pc1_sorted)), pc1_sorted.values, color='coral', edgecolor='black')\n",
    "axes[0].set_yticks(range(len(pc1_sorted)))\n",
    "axes[0].set_yticklabels(pc1_sorted.index)\n",
    "axes[0].set_xlabel('Loading Value', fontsize=12)\n",
    "axes[0].set_title(f'Feature Loadings on PC1\\n({pca_2d.explained_variance_ratio_[0]:.2%} variance)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# PC2 loadings\n",
    "pc2_sorted = components_df['PC2'].sort_values()\n",
    "axes[1].barh(range(len(pc2_sorted)), pc2_sorted.values, color='skyblue', edgecolor='black')\n",
    "axes[1].set_yticks(range(len(pc2_sorted)))\n",
    "axes[1].set_yticklabels(pc2_sorted.index)\n",
    "axes[1].set_xlabel('Loading Value', fontsize=12)\n",
    "axes[1].set_title(f'Feature Loadings on PC2\\n({pca_2d.explained_variance_ratio_[1]:.2%} variance)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Final Summary and Conclusions\n",
    "\n",
    "Let's summarize all findings and model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PROJECT SUMMARY - WINE QUALITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š DATASET:\")\n",
    "print(f\"  â€¢ Total samples: {df.shape[0]}\")\n",
    "print(f\"  â€¢ Features: {len(numeric_features)} (including engineered features)\")\n",
    "print(f\"  â€¢ Target: Wine Quality (3-8 scale)\")\n",
    "print(f\"  â€¢ Train/Test split: 80% / 20%\")\n",
    "\n",
    "print(\"\\nðŸ”¬ REGRESSION MODELS (Predicting Quality Score):\")\n",
    "print(regression_results.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸŽ¯ CLASSIFICATION MODELS (High Quality vs Low Quality):\")\n",
    "print(classification_results.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ” UNSUPERVISED LEARNING:\")\n",
    "print(f\"  â€¢ K-Means Clustering: {optimal_k} clusters identified\")\n",
    "print(f\"  â€¢ PCA: First 2 components explain {cumulative_variance[1]:.2%} of variance\")\n",
    "print(f\"  â€¢ PCA: First 5 components explain {cumulative_variance[4]:.2%} of variance\")\n",
    "\n",
    "print(\"\\nâœ¨ KEY INSIGHTS:\")\n",
    "print(\"  1. Alcohol content is the strongest predictor of wine quality\")\n",
    "print(\"  2. Volatile acidity negatively correlates with quality\")\n",
    "print(\"  3. Random Forest models perform best for both regression and classification\")\n",
    "print(\"  4. Hyperparameter tuning improves model performance\")\n",
    "print(\"  5. Feature engineering (ratios, interactions) adds predictive power\")\n",
    "print(\"  6. Most wines cluster into 2-3 distinct quality groups\")\n",
    "print(\"  7. Dimensionality reduction via PCA preserves most variance with fewer features\")\n",
    "\n",
    "print(\"\\nðŸŽ“ RECOMMENDATIONS:\")\n",
    "print(\"  â€¢ For production: Use Random Forest Classifier (tuned) - Best accuracy\")\n",
    "print(\"  â€¢ For interpretability: Use Decision Tree - Easy to explain\")\n",
    "print(\"  â€¢ For speed: Use KNN or Naive Bayes - Fast predictions\")\n",
    "print(\"  â€¢ Consider ensemble methods for further improvement\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Project\n",
    "\n",
    "This comprehensive analysis covered:\n",
    "- âœ… Data loading and exploration\n",
    "- âœ… Data cleaning and preprocessing\n",
    "- âœ… Extensive exploratory data analysis (EDA)\n",
    "- âœ… Feature engineering\n",
    "- âœ… Regression models (Decision Tree, Random Forest)\n",
    "- âœ… Classification models (KNN, Naive Bayes, Decision Tree, Random Forest with hyperparameter tuning)\n",
    "- âœ… Unsupervised learning (K-Means clustering, PCA)\n",
    "- âœ… Model comparison and evaluation\n",
    "- âœ… Comprehensive visualizations\n",
    "\n",
    "**Thank you for following along! ðŸ·**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
